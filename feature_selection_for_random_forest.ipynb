{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select temperatures using linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests as rq\n",
    "import pandas as pd\n",
    "import os\n",
    "import warnings\n",
    "from datetime import timedelta\n",
    "from sklearn import preprocessing\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from scipy.stats import f\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor as RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "end = '20190918T23-05'\n",
    "start = '20130918T23-05'\n",
    "key = '8a6315646d5695061696c71a041c42c0'\n",
    "series_id = 'EBA.NW-ALL.D.HL'\n",
    "data = rq.get(\"http://api.eia.gov/series/?api_key={}&series_id={}&start={}&end={}\".format(key, series_id, start, end))\n",
    "hourly_demand = data.json()['series'][0]['data']\n",
    "electricity_demand = pd.DataFrame(hourly_demand, columns=['datetime','usage'])\n",
    "# electricity_demand = pd.read_csv('electricity_data/electricity_demand.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "electricity_demand['datetime'] = pd.to_datetime(electricity_demand['datetime'], format='%Y%m%dT%H-%M')\n",
    "electricity_demand['time'] = electricity_demand['datetime'].apply(lambda x:x.hour)\n",
    "electricity_demand['month'] = electricity_demand['datetime'].apply(lambda x:x.month)\n",
    "electricity_demand['weekday'] = electricity_demand['datetime'].apply(lambda x:x.weekday())\n",
    "electricity_demand['year'] = electricity_demand['datetime'].apply(lambda x:x.year)\n",
    "electricity_demand['day'] = electricity_demand['datetime'].apply(lambda x:x.day)\n",
    "\n",
    "# Normalize the usage\n",
    "electricity_demand['normalized_usage'] = preprocessing.MinMaxScaler().fit_transform(electricity_demand['usage'].values.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#weather_2013 = pd.read_csv('electricity_data/hourly_temperature_2013.csv')\n",
    "#weather_2015 = pd.read_csv('electricity_data/hourly_temperature_2015.csv')\n",
    "#weather_2016 = pd.read_csv('electricity_data/hourly_temperature_2016.csv')\n",
    "#weather = pd.concat([weather_2013, weather_2015, weather_2016])\n",
    "\n",
    "# Other weather data.\n",
    "# temp_dir = 'electricity_data/temperatures/'\n",
    "temp_dir = 'electricity_data/NW/'\n",
    "# stations = [72243012960, 72259003927, 72253012921]\n",
    "stations = []\n",
    "cities = []\n",
    "# weather = pd.DataFrame()\n",
    "\n",
    "for file in os.listdir(temp_dir):\n",
    "    if file.endswith('.csv') and file != \"Bend.csv\":\n",
    "        cities.append(file.split('.')[0])\n",
    "        #temp_df = pd.read_csv(temp_dir+file)\n",
    "        #stations.append(temp_df.head(1)['STATION'].values[0])\n",
    "        #weather = pd.concat([weather, temp_df], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AWND is abondoned, containing 796753 NaN\n",
      "BackupDirection is abondoned, containing 430135 NaN\n",
      "BackupDistance is abondoned, containing 430135 NaN\n",
      "BackupDistanceUnit is abondoned, containing 430135 NaN\n",
      "BackupElements is abondoned, containing 371586 NaN\n",
      "BackupElevation is abondoned, containing 371586 NaN\n",
      "BackupElevationUnit is abondoned, containing 797402 NaN\n",
      "BackupEquipment is abondoned, containing 430135 NaN\n",
      "BackupLatitude is abondoned, containing 371586 NaN\n",
      "BackupLongitude is abondoned, containing 371586 NaN\n",
      "BackupName is abondoned, containing 371586 NaN\n",
      "CDSD is abondoned, containing 796754 NaN\n",
      "CLDD is abondoned, containing 796754 NaN\n",
      "DSNW is abondoned, containing 796846 NaN\n",
      "DailyAverageDewPointTemperature is abondoned, containing 777675 NaN\n",
      "DailyAverageDryBulbTemperature is abondoned, containing 777364 NaN\n",
      "DailyAverageRelativeHumidity is abondoned, containing 777649 NaN\n",
      "DailyAverageSeaLevelPressure is abondoned, containing 777702 NaN\n",
      "DailyAverageStationPressure is abondoned, containing 777393 NaN\n",
      "DailyAverageWetBulbTemperature is abondoned, containing 777675 NaN\n",
      "DailyAverageWindSpeed is abondoned, containing 777358 NaN\n",
      "DailyCoolingDegreeDays is abondoned, containing 777364 NaN\n",
      "DailyDepartureFromNormalAverageTemperature is abondoned, containing 777364 NaN\n",
      "DailyHeatingDegreeDays is abondoned, containing 777364 NaN\n",
      "DailyMaximumDryBulbTemperature is abondoned, containing 777364 NaN\n",
      "DailyMinimumDryBulbTemperature is abondoned, containing 777364 NaN\n",
      "DailyPeakWindDirection is abondoned, containing 777399 NaN\n",
      "DailyPeakWindSpeed is abondoned, containing 777393 NaN\n",
      "DailyPrecipitation is abondoned, containing 777358 NaN\n",
      "DailySnowDepth is abondoned, containing 781368 NaN\n",
      "DailySnowfall is abondoned, containing 780545 NaN\n",
      "DailySustainedWindDirection is abondoned, containing 777357 NaN\n",
      "DailySustainedWindSpeed is abondoned, containing 777357 NaN\n",
      "DailyWeather is abondoned, containing 787192 NaN\n",
      "HDSD is abondoned, containing 796760 NaN\n",
      "HTDD is abondoned, containing 796754 NaN\n",
      "HeavyFog is abondoned, containing 797402 NaN\n",
      "HourlyAltimeterSetting is abondoned, containing 109759 NaN\n",
      "HourlyPrecipitation is abondoned, containing 279016 NaN\n",
      "HourlyPresentWeatherType is abondoned, containing 665449 NaN\n",
      "HourlyPressureChange is abondoned, containing 548429 NaN\n",
      "HourlyPressureTendency is abondoned, containing 548429 NaN\n",
      "HourlySeaLevelPressure is abondoned, containing 226761 NaN\n",
      "HourlySkyConditions is abondoned, containing 78672 NaN\n",
      "HourlyWindGustSpeed is abondoned, containing 724767 NaN\n",
      "MonthlyAverageRH is abondoned, containing 797402 NaN\n",
      "MonthlyDaysWithGT001Precip is abondoned, containing 796753 NaN\n",
      "MonthlyDaysWithGT010Precip is abondoned, containing 796754 NaN\n",
      "MonthlyDaysWithGT32Temp is abondoned, containing 796753 NaN\n",
      "MonthlyDaysWithGT90Temp is abondoned, containing 796753 NaN\n",
      "MonthlyDaysWithLT0Temp is abondoned, containing 796753 NaN\n",
      "MonthlyDaysWithLT32Temp is abondoned, containing 796753 NaN\n",
      "MonthlyDepartureFromNormalAverageTemperature is abondoned, containing 796753 NaN\n",
      "MonthlyDepartureFromNormalCoolingDegreeDays is abondoned, containing 796754 NaN\n",
      "MonthlyDepartureFromNormalHeatingDegreeDays is abondoned, containing 796987 NaN\n",
      "MonthlyDepartureFromNormalMaximumTemperature is abondoned, containing 796987 NaN\n",
      "MonthlyDepartureFromNormalMinimumTemperature is abondoned, containing 796753 NaN\n",
      "MonthlyDepartureFromNormalPrecipitation is abondoned, containing 796800 NaN\n",
      "MonthlyDewpointTemperature is abondoned, containing 797402 NaN\n",
      "MonthlyGreatestPrecip is abondoned, containing 796754 NaN\n",
      "MonthlyGreatestPrecipDate is abondoned, containing 796756 NaN\n",
      "MonthlyGreatestSnowDepth is abondoned, containing 796883 NaN\n",
      "MonthlyGreatestSnowDepthDate is abondoned, containing 797189 NaN\n",
      "MonthlyGreatestSnowfall is abondoned, containing 796856 NaN\n",
      "MonthlyGreatestSnowfallDate is abondoned, containing 797092 NaN\n",
      "MonthlyMaxSeaLevelPressureValue is abondoned, containing 796753 NaN\n",
      "MonthlyMaxSeaLevelPressureValueDate is abondoned, containing 796753 NaN\n",
      "MonthlyMaxSeaLevelPressureValueTime is abondoned, containing 796753 NaN\n",
      "MonthlyMaximumTemperature is abondoned, containing 796753 NaN\n",
      "MonthlyMeanTemperature is abondoned, containing 796753 NaN\n",
      "MonthlyMinSeaLevelPressureValue is abondoned, containing 796753 NaN\n",
      "MonthlyMinSeaLevelPressureValueDate is abondoned, containing 796753 NaN\n",
      "MonthlyMinSeaLevelPressureValueTime is abondoned, containing 796753 NaN\n",
      "MonthlyMinimumTemperature is abondoned, containing 796753 NaN\n",
      "MonthlySeaLevelPressure is abondoned, containing 796764 NaN\n",
      "MonthlyStationPressure is abondoned, containing 796764 NaN\n",
      "MonthlyTotalLiquidPrecipitation is abondoned, containing 796800 NaN\n",
      "MonthlyTotalSnowfall is abondoned, containing 797092 NaN\n",
      "MonthlyWetBulb is abondoned, containing 797402 NaN\n",
      "NormalsCoolingDegreeDay is abondoned, containing 796753 NaN\n",
      "NormalsHeatingDegreeDay is abondoned, containing 796753 NaN\n",
      "ShortDurationEndDate005 is abondoned, containing 796881 NaN\n",
      "ShortDurationEndDate010 is abondoned, containing 796881 NaN\n",
      "ShortDurationEndDate015 is abondoned, containing 796881 NaN\n",
      "ShortDurationEndDate020 is abondoned, containing 796881 NaN\n",
      "ShortDurationEndDate030 is abondoned, containing 796881 NaN\n",
      "ShortDurationEndDate045 is abondoned, containing 796881 NaN\n",
      "ShortDurationEndDate060 is abondoned, containing 796881 NaN\n",
      "ShortDurationEndDate080 is abondoned, containing 796881 NaN\n",
      "ShortDurationEndDate100 is abondoned, containing 796881 NaN\n",
      "ShortDurationEndDate120 is abondoned, containing 796881 NaN\n",
      "ShortDurationEndDate150 is abondoned, containing 796881 NaN\n",
      "ShortDurationEndDate180 is abondoned, containing 796881 NaN\n",
      "ShortDurationPrecipitationValue005 is abondoned, containing 796881 NaN\n",
      "ShortDurationPrecipitationValue010 is abondoned, containing 796881 NaN\n",
      "ShortDurationPrecipitationValue015 is abondoned, containing 796881 NaN\n",
      "ShortDurationPrecipitationValue020 is abondoned, containing 796881 NaN\n",
      "ShortDurationPrecipitationValue030 is abondoned, containing 796881 NaN\n",
      "ShortDurationPrecipitationValue045 is abondoned, containing 796881 NaN\n",
      "ShortDurationPrecipitationValue060 is abondoned, containing 796881 NaN\n",
      "ShortDurationPrecipitationValue080 is abondoned, containing 796881 NaN\n",
      "ShortDurationPrecipitationValue100 is abondoned, containing 796881 NaN\n",
      "ShortDurationPrecipitationValue120 is abondoned, containing 796881 NaN\n",
      "ShortDurationPrecipitationValue150 is abondoned, containing 796881 NaN\n",
      "ShortDurationPrecipitationValue180 is abondoned, containing 796881 NaN\n",
      "Sunrise is abondoned, containing 776633 NaN\n",
      "Sunset is abondoned, containing 776633 NaN\n",
      "TStorms is abondoned, containing 797402 NaN\n",
      "WindEquipmentChangeDate is abondoned, containing 279045 NaN\n"
     ]
    }
   ],
   "source": [
    "# Many columns has very few useful rows. Collect only useful columns.\n",
    "columns = weather.columns\n",
    "\n",
    "useful_columns = []\n",
    "for column in columns:\n",
    "    if sum(weather[column].isna()) > 30000:\n",
    "        # There are more than 10000 nan\n",
    "        print('{} is abondoned, containing {} NaN'.format(column, sum(weather[column].isna())))\n",
    "    else:\n",
    "        useful_columns.append(column)\n",
    "        \n",
    "weather = weather[useful_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def round_time(date_time):\n",
    "    minute = date_time.minute\n",
    "    if minute > 30:\n",
    "        return date_time+timedelta(hours=1)\n",
    "    else:\n",
    "        return date_time\n",
    "    \n",
    "def parse_temperature(s):\n",
    "    try:\n",
    "        return float(s)\n",
    "    except:\n",
    "        try:\n",
    "            return float(re.split('[a-z]+', s)[0])\n",
    "        except:\n",
    "            return np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Round the date info\n",
    "weather['datetime'] = pd.to_datetime(weather['DATE'], format='%Y-%m-%dT%H:%M:%S')\n",
    "weather['date'] = weather['datetime'].apply(lambda date_time: round_time(date_time))\n",
    "weather['time'] = weather['date'].apply(lambda x:x.hour)\n",
    "weather['month'] = weather['date'].apply(lambda x:x.month)\n",
    "weather['day'] = weather['date'].apply(lambda x:x.day)\n",
    "# weather['weekday'] = weather['date'].apply(lambda x:x.weekday())\n",
    "weather['year'] = weather['date'].apply(lambda x:x.year)\n",
    "weather['HourlyDryBulbTemperature'] = weather['HourlyDryBulbTemperature'].apply(lambda s:parse_temperature(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get rid of nan and duplicated values\n",
    "match_columns = ['year', 'month', 'day', 'time']\n",
    "useful_columns = ['STATION', 'DATE', 'HourlyDryBulbTemperature', 'month', 'day', 'time', 'year']\n",
    "weather = weather.loc[weather['HourlyDryBulbTemperature'].notna(),:]\n",
    "weather = weather[useful_columns].groupby(match_columns+['STATION'], as_index=False).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "72569024089\n",
      "72464093058\n"
     ]
    }
   ],
   "source": [
    "# bad_station = [72232403071]\n",
    "bad_station = [72063800224]\n",
    "df_joined = electricity_demand\n",
    "\n",
    "\n",
    "for station in stations:\n",
    "    if station not in bad_station:\n",
    "        df_tmp = weather.loc[weather['STATION'] == station, :]\n",
    "        if sum(df_tmp['HourlyDryBulbTemperature'].isna()) != 0:\n",
    "            print(station)\n",
    "        old_num_rows = len(df_joined)\n",
    "        df_joined = df_joined.merge(df_tmp, how='inner', left_on=match_columns, right_on=match_columns)\n",
    "        new_num_rows = len(df_joined)\n",
    "        if new_num_rows < old_num_rows-100:\n",
    "            print(station)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_column_names = ['datetime', 'usage', 'time', 'month', 'weekday', 'year', 'day',\n",
    "       'normalized_usage', 'STATION_1', 'T_1',\n",
    "       'STATION_2', 'T_2', 'STATION_3', 'T_3', 'STATION_4', 'T_4',\n",
    "       'STATION_5', 'T_5', 'STATION_6', 'T_6', 'STATION_7', 'T_7',\n",
    "       'STATION_8', 'T_8', 'STATION_9', 'T_9', 'STATION_10', 'T_10',\n",
    "       'STATION_11', 'T_11', 'STATION_12', 'T_12', 'STATION_13', 'T_13']\n",
    "df_joined.columns = new_column_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_selection_using_LR(X, y):\n",
    "    num_features = X.shape[1]\n",
    "    features_ordered = []\n",
    "    RSS_0 = np.sum((y-np.mean(y))**2)\n",
    "    min_p = 0\n",
    "    max_f = 1\n",
    "    while max_f > 0.5: # The Null hypothesis is rejected.\n",
    "        max_f = -np.inf\n",
    "        min_p = np.inf # Find the next figure producing the maximum p value.\n",
    "        next_feature = -1\n",
    "        min_RSS = np.inf\n",
    "        \n",
    "        for ind in np.arange(num_features):\n",
    "            if ind in features_ordered:\n",
    "                continue\n",
    "            y_pred = LinearRegression().fit(X[:,features_ordered+[ind]],y).predict(X[:,features_ordered+[ind]])\n",
    "            RSS_1 = np.sum((y-y_pred)**2)\n",
    "            \n",
    "            degree = X.shape[0]-len(features_ordered)-1\n",
    "            f_stat = (RSS_0-RSS_1)*degree/RSS_1 # The f statistics\n",
    "            p_value = f.sf(f_stat, 1, degree)\n",
    "            p_value = 2*(p_value if p_value<0.5 else 1-p_value)\n",
    "            '''\n",
    "            if p_value < min_p:\n",
    "                min_p = p_value\n",
    "                next_feature = ind\n",
    "                min_RSS = RSS_1\n",
    "            '''\n",
    "                \n",
    "            if f_stat > max_f:\n",
    "                max_f = f_stat\n",
    "                next_feature = ind\n",
    "                min_RSS = RSS_1\n",
    "                \n",
    "        features_ordered.append(next_feature)\n",
    "        # print('Feature {} is included with a P value {}.'.format(next_feature, min_p))\n",
    "        print('Feature {} is included with a f statistics {}.'.format(next_feature, max_f))\n",
    "        RSS_0 = min_RSS\n",
    "    return features_ordered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature 1 is included with a f statistics 1831.0221428624195.\n",
      "Feature 4 is included with a f statistics 2440.307131065897.\n",
      "Feature 8 is included with a f statistics 336.51340248871685.\n",
      "Feature 6 is included with a f statistics 243.19692118225814.\n",
      "Feature 12 is included with a f statistics 126.83928628787747.\n",
      "Feature 2 is included with a f statistics 114.96709620810812.\n",
      "Feature 10 is included with a f statistics 124.66488764830426.\n",
      "Feature 0 is included with a f statistics 124.36233087311439.\n",
      "Feature 3 is included with a f statistics 168.2604706749979.\n",
      "Feature 11 is included with a f statistics 28.70938451047781.\n",
      "Feature 9 is included with a f statistics 15.583150482260166.\n",
      "Feature 7 is included with a f statistics 0.5619519033995625.\n",
      "Feature 5 is included with a f statistics 0.04115025845023452.\n"
     ]
    }
   ],
   "source": [
    "temp_features = []\n",
    "\n",
    "for i in np.arange(1,14):\n",
    "    temp_features.append('T_'+str(i))\n",
    "    \n",
    "features_ordered = feature_selection_using_LR(df_joined[temp_features].values, df_joined['usage'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['datetime', 'usage', 'time', 'month', 'weekday', 'year', 'day',\n",
       "       'normalized_usage', 'STATION_1', 'T_1', 'STATION_2', 'T_2', 'STATION_3',\n",
       "       'T_3', 'STATION_4', 'T_4', 'STATION_5', 'T_5', 'STATION_6', 'T_6',\n",
       "       'STATION_7', 'T_7', 'STATION_8', 'T_8', 'STATION_9', 'T_9',\n",
       "       'STATION_10', 'T_10', 'STATION_11', 'T_11', 'STATION_12', 'T_12',\n",
       "       'STATION_13', 'T_13'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_joined.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Redmond\n",
      "Spokane\n",
      "Eugene\n",
      "Boise\n",
      "Portland\n",
      "Billings\n",
      "Pueblo\n",
      "Denver\n",
      "Casper\n",
      "Great_Falls\n",
      "Seattle\n",
      "Pocatello\n",
      "Salt_Lake_City\n"
     ]
    }
   ],
   "source": [
    "for ind in features_ordered:\n",
    "    print(cities[ind])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['T_1' 'T_2' 'T_5' 'T_12' 'T_4' 'T_7' 'T_6' 'T_9' 'T_8' 'T_3' 'T_11'\n",
      " 'T_10']\n"
     ]
    }
   ],
   "source": [
    "# print the stations that are important\n",
    "print(np.asarray(temp_features)[features_ordered[:-1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([72243012960, 72259003927, 72363023047, 72092600313, 72242012923,\n",
       "       72250612959, 72259453909, 72254013904, 72096513910, 72253012921,\n",
       "       72267023042, 72232403071])"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.asarray(stations)[features_ordered[:-1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'Houston', 'Dallas', 'Amarillo', 'Beaumont', 'Galveston', 'Macallen', 'Fort Worth', 'Austin', 'Abilene', 'San Antonio', 'Lubbock', 'Midland'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature selection using random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attenuate(X,Y,beta,column):\n",
    "    # The first 24 hour will be used as warm up.\n",
    "    X_new = (1-beta)*X[24:, column:].copy()\n",
    "    # X_new += 0.9*X[23:-1,column:]+0.9*X[22:-2,column:]+0.9*X[21:-3,column:]+0.9*X[20:-4,column:]# +0.9*X[19:-5,column:]+0.9*X[18:-6,column:]\n",
    "    # for h in np.arange(1, 6):\n",
    "        # X_new = np.concatenate((X_new, X[24-h:-1*h,column:]), axis=1)\n",
    "    for h in np.arange(1, 24):\n",
    "        X_new += beta**h*(1-beta)*X[24-h:-1*h, column:]\n",
    "    return np.concatenate((X[24:,:column], X_new), axis=1), Y[24:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Smooth the temperature.\n",
    "beta = 0.84\n",
    "column = 3\n",
    "all_features = ['time','weekday','month']+temp_features\n",
    "X_rf, y_rf = df_joined[all_features].values, df_joined['usage'].values\n",
    "X_rf, y_rf = attenuate(X_rf, y_rf, beta, column)\n",
    "\n",
    "num_samples = len(y_rf)\n",
    "split_point = int(num_samples*2/3)\n",
    "train_ind = np.arange(split_point)\n",
    "valid_ind = np.arange(split_point, num_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import base\n",
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "class elec_forcast(base.BaseEstimator):\n",
    "    \n",
    "    def __init__(self, n_estimators=700, min_samples_split=4, min_samples_leaf=2, random_state=10):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.min_samples_leaf = min_samples_leaf\n",
    "        self.random_state = random_state\n",
    "        self.rf = RF(n_estimators=self.n_estimators, min_samples_split=self.min_samples_split, \\\n",
    "                    min_samples_leaf=self.min_samples_leaf, \\\n",
    "                    random_state=self.random_state)\n",
    "        \n",
    "    def get_params(self, deep=True):\n",
    "        return {\"n_estimators\": self.n_estimators,\n",
    "                \"min_samples_split\":self.min_samples_split,\n",
    "                \"min_samples_leaf\": self.min_samples_leaf,\n",
    "                \"random_state\": self.random_state}\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        self.rf.fit(X,y)\n",
    "        \n",
    "    def predict(self, X):\n",
    "        return self.rf.predict(X)        \n",
    "        \n",
    "    def score(self, X, Y_true):\n",
    "        Y_pred = self.predict(X)\n",
    "        Y_pred = LinearRegression().fit(Y_pred.reshape(-1,1), Y_true.reshape(-1,1)).predict(Y_pred.reshape(-1,1))\n",
    "        return r2_score(Y_true, Y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_selection_using_rf(X_rf, y_rf, train_ind, vaild_ind):\n",
    "    grid = {'n_estimators': [50],\n",
    "       'min_samples_split': [4,6],\n",
    "       'min_samples_leaf': [1,2],\n",
    "       'random_state': [10]}\n",
    "    \n",
    "    num_features = X_rf.shape[1]\n",
    "    features_ordered = [0, 8, 1, 4, 2, 3, 9, 5, 10] #[]\n",
    "    r2_diff = 1\n",
    "    old_r2 = 0\n",
    "    while r2_diff > -1: # The Null hypothesis is rejected.\n",
    "        # Find the next feature producing the r2 score.\n",
    "        next_feature = -1\n",
    "        max_r2 = -np.inf\n",
    "        \n",
    "        for ind in np.arange(num_features):\n",
    "            if ind in features_ordered:\n",
    "                continue\n",
    "            # Generate a r2 score for the validation set after gridsearch.\n",
    "            search = GridSearchCV(elec_forcast(), grid, cv=[(train_ind, valid_ind)], n_jobs=4)\n",
    "            search.fit(X_rf[:,features_ordered+[ind]], y_rf)            \n",
    "            \n",
    "            r2 = search.best_score_\n",
    "            \n",
    "            if r2 > max_r2:\n",
    "                max_r2 = r2\n",
    "                next_feature = ind\n",
    "                \n",
    "        features_ordered.append(next_feature)\n",
    "        print('Feature {} is included with a r2 score {}.'.format(next_feature, max_r2))\n",
    "        r2_diff = max_r2 - old_r2\n",
    "        old_r2 = max_r2\n",
    "    return features_ordered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature 6 is included with a r2 score 0.8355740679371391.\n",
      "Feature 13 is included with a r2 score 0.834832680349859.\n",
      "Feature 15 is included with a r2 score 0.8345312661411547.\n",
      "Feature 12 is included with a r2 score 0.8337528874895903.\n",
      "Feature 14 is included with a r2 score 0.833028331338397.\n",
      "Feature 11 is included with a r2 score 0.8317148652439311.\n",
      "Feature 7 is included with a r2 score 0.8289771327884842.\n",
      "Feature -1 is included with a r2 score -inf.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0, 8, 1, 4, 2, 3, 9, 5, 10, 6, 13, 15, 12, 14, 11, 7, -1]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_selection_using_rf(X_rf, y_rf, train_ind, valid_ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_ordered = [0, 8, 1, 4, 2, 3, 9, 5, 10, 6, 13, 15, 12, 14, 11, 7, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Salt_Lake_City\n",
      "Redmond\n",
      "Denver\n",
      "Boise\n",
      "Billings\n",
      "Pocatello\n",
      "Casper\n",
      "Pueblo\n",
      "Portland\n",
      "Seattle\n",
      "Great_Falls\n",
      "Eugene\n",
      "Spokane\n"
     ]
    }
   ],
   "source": [
    "for ind in features_ordered:\n",
    "    if ind > 2:\n",
    "        print(cities[ind-3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 4, 8, 6, 12, 2, 10, 0, 3, 11, 9, 7, 5]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_ordered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
